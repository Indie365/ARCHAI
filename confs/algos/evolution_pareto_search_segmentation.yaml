__include__: "../datasets/cifar10.yaml" # default dataset settings are for cifar

common:
  experiment_name: 'throwaway' # you should supply from command line
  experiment_desc: 'throwaway'
  logdir: '~/logdir'
  log_prefix: 'log' # prefix for log files that will becreated (log.log and log.yaml), no log files if ''
  log_level: 20 # logging.INFO
  backup_existing_log_file: False # should we overwrite existing log file without making a copy?
  yaml_log: True # if True, structured logs as yaml are also generated
  seed: 2.0
  tb_enable: False # if True then TensorBoard logging is enabled (may impact perf)
  tb_dir: '$expdir/tb' # path where tensorboard logs would be stored
  checkpoint:
    filename: '$expdir/checkpoint.pth'
    freq: 10

  # reddis address of Ray cluster. Use None for single node run
  # otherwise it should something like host:6379. Make sure to run on head node:
  # "ray start --head --redis-port=6379"
  redis: null
  apex: # this is overriden in search and eval individually
    enabled: False # global switch to disable everything apex
    distributed_enabled: True # enable/disable distributed mode
    mixed_prec_enabled: True # switch to disable amp mixed precision
    gpus: '' # use GPU IDs specified here (comma separated), if '' then use all GPUs
    opt_level: 'O2' # optimization level for mixed precision
    bn_fp32: True # keep BN in fp32
    loss_scale: "dynamic" # loss scaling mode for mixed prec, must be string reprenting floar ot "dynamic"
    sync_bn: False # should be replace BNs with sync BNs for distributed model
    scale_lr: True # enable/disable distributed mode
    min_world_size: 0 # allows to confirm we are indeed in distributed setting
    detect_anomaly: False # if True, PyTorch code will run 6X slower
    seed: '_copy: /common/seed'
    ray:
      enabled: False # initialize ray. Note: ray cannot be used if apex distributed is enabled
      local_mode: False # if True then ray runs in serial mode

  # smoke_test: False
  # only_eval: False
  # resume: True

dataset: {} # default dataset settings comes from __include__ on the top

nas:
  search:
    init_num_models: 100 # initial random models to seed the search 
    init_architectures_from_dir: '' # path to directory with yaml files to manually seed the search
    num_iters: 20 # number of pareto frontier search iterations 
    num_random_mix: 20 # how many random models to add to the parent mixture
    max_unseen_population: 100 # restrict the new population at each iteration to prevent blowing up of models
    min_mac: 100_000000 # models with mac below will be rejected
    max_mac: 7000_000000 # models with mac above will be rejected
    min_layers: 5
    max_layers: 14
    max_downsample_factor: 16 # must be one of {2, 4, 8, 16}
    skip_connections: True
    max_skip_connection_length: 3
    max_scale_delta: 1 # must be one of {1, 2, 3}
    max_post_upsample_layers: 3
    min_base_channels: 8
    max_base_channels: 36
    base_channels_binwidth: 8
    min_delta_channels: 8
    max_delta_channels: 48
    delta_channels_binwidth: 8
    mutations_per_parent: 1
    num_crossovers: 0
    op_subset: ''
    downsample_prob_ratio: 1.25
  
    objectives:
      f1: 
        enabled: true # Has to be true for now
        min: 0.0
        max: 1.0

      latency: 
        enabled: true
        min: 0.0
        max: 10000.0 # Microseconds

      memory: 
        enabled: true
        min: 0.0
        max: 400.0 # Megabytes

      # Penalty score that measures the proportion of nodes that violate a set of constraints
      soft_constraints_penalty:
        enabled: false
        allowed_ops: [conv3x3, mbconv3x3_e1, mbconv3x3_e2]
        allowed_scales: [1, 2, 4, 8, 16]
        allowed_channels: [64, 128, 245, 512]
        min: 0.0
        max: 10.0 # Maximum penalty. Everything above this threshold will be discarded

    crowd_sorting:
      initialization: false
      random_mix: false
      mutation: false
      oversampling_factor: 3

    use_remote_benchmark: False
    remote_benchmark_config:
      connection_string_env_var_name: ''
      blob_container_name: ''
      table_name: ''
      partition_key: ''
      overwrite: false
      patience: 5 # how many times to wait if the number of evaluated models does not increase
      check_interval: 120 # how often (seconds) to check if the number of evaluated models has increased 

    loader:
      batch_size: 16 # TODO: batch size 16 has lr 2e-4. can we increase batch size? what lr?
      dataset:
        _copy: '/dataset'
    trainer:
      gpus_per_job: 0.2 # fraction of GPUs to use per job
      evaluate_for_steps: 5625
      val_size: 2000
      img_size: 256
      augmentation: 'none'
      lr: 0.0002
      criterion_name: 'ce'
      val_check_interval: 1.0 # how often to evaluate validation accuracy per epoch
      lr_exp_decay_gamma: 0.973435286
