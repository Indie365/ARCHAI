__include__: 'darts.yaml' # just use darts defaults

# dataset:
#   name: 'cifar10'
#   n_classes: 10
#   channels: 3 # number of channels in image
#   max_batches: -1 # if >= 0 then only these many batches are generated (useful for debugging)
#   storage_name: 'cifar-10-batches-py' # name of folder or tar file to copy from cloud storage

# dataset:
#   name: 'cifar100'
#   n_classes: 100
#   channels: 3 # number of channels in image
#   max_batches: -1 # if >= 0 then only these many batches are generated (useful for debugging)
#   storage_name: 'cifar-100-python' # name of folder or tar file to copy from cloud storage

# dataset:
#   name: 'ImageNet16-120'
#   n_classes: 120
#   channels: 3 # number of channels in image
#   max_batches: -1 # if >= 0 then only these many batches are generated (useful for debugging)
#   storage_name: 'imagenet16' # name of folder or tar file to copy from cloud storage

dataset:
  name: 'synthetic_cifar10'
  n_classes: 10
  channels: 3 # number of channels in image
  max_batches: -1 # if >= 0 then only these many batches are generated (useful for debugging)
  storage_name: 'synthetic_cifar10' # name of folder or tar file to copy from cloud storage


common:
  experiment_name: 'VisualFeaturesWithLinearNet'

nas:
  eval:
    pixels_per_hog_cell: 8 # for ImageNet16-120: 4, for cifar10: 8
    feature_len: 324 # for ImageNet16-120: 324, for cifar10: 324
    loader:
      train_batch: 1024
      test_batch: 4096
      cutout: 0
      val_ratio: 0.0
      # dataset:
      #   max_batches: 2
    trainer:
      epochs: 200
      logger_freq: 1000
      drop_path_prob: 0.0
      grad_clip: 0.0
      aux_weight: 0.0
      optimizer:
        type: 'sgd'
        lr: 0.5 # init learning rate
        decay: 3.0e-4 # pytorch default is 0.0
        momentum: 0.9 # pytorch default is 0.0
        nesterov: False # pytorch default is False
        decay_bn: .NaN # if NaN then same as decay otherwise apply different decay to BN layers
      lr_schedule:
        type: 'cosine'
        min_lr: 0.001 # min learning rate to be set in eta_min param of scheduler
        warmup:  # increases LR for 0 to current in specified epochs and then hands over to main scheduler
          multiplier: 1
          epochs: 0 # 0 disables warmup
